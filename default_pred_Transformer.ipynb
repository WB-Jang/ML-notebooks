{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7bfd47-1227-4325-bcd6-372aeadbb07c",
   "metadata": {},
   "source": [
    "# Transformer를 활용한 부도 예측 모델링\n",
    "1. 트랜스포머 인코더 구조를 활용하기 위하여, 수치형 데이터와 범주형 데이터 모두 32차원 벡터로 변환\n",
    "2. multi-head-self attention 통과 후 FC layer 통과\n",
    "3. 리소스 한계로 epoch 10에 머물렀기에 성능이 부족할 수 있지만, epoch가 10에 도달하는 과정에서 성능 개선의 폭이 줄어들지 않았기에, 추가 성능 향상 가능성 있음\n",
    "4. 추가적인 대량 데이터가 들어오면, 성능 향상이 이루어질 가능성이 있고, 추후에는 텍스트 데이터를 추가하는 등의 확장성이 뛰어난 점에 착안하여 트랜스포머 아키텍쳐 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe25a9-86e3-4c20-83e6-6d31b3ebe3f1",
   "metadata": {},
   "source": [
    "# 컬럼 명에 Flag가 들어가거나, 유니크 값이 N개 미만인 경우, 사실상 범주형 정보로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11937989-8bbf-41aa-a568-a9fca954761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Data_EDA = pd.read_csv('../../Data/home-credit-default-risk/application_train.csv',index_col='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5de3ad0-a58f-4318-bdf5-a4f319389100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형(유니크값 < 10): ['TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n",
      "수치형(유니크값 >= 10): ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.api.types as ptypes\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "cat_cols_by_cardinality = []\n",
    "num_cols_by_cardinality = []\n",
    "\n",
    "for col in Data_EDA.columns:\n",
    "\n",
    "    # 고유값 개수 확인\n",
    "    unique_count = Data_EDA[col].nunique()\n",
    "    \n",
    "    if 'flag' in col.lower():\n",
    "        cat_cols_by_cardinality.append(col)\n",
    "    elif 'amt' in col.lower():\n",
    "        num_cols_by_cardinality.append(col)\n",
    "\n",
    "    # 실수형/정수형이면 일단 수치형으로 분류하되, \n",
    "    # 만약 유니크 값이 작은 범주 느낌이라면 cat_cols_by_cardinality 로 옮길 수도 있음\n",
    "    elif ptypes.is_numeric_dtype(Data_EDA[col]):\n",
    "        if unique_count < threshold:\n",
    "            cat_cols_by_cardinality.append(col)\n",
    "        else:\n",
    "            num_cols_by_cardinality.append(col)\n",
    "    else:\n",
    "        # 문자인 경우 범주형으로 분류\n",
    "        cat_cols_by_cardinality.append(col)\n",
    "\n",
    "print(\"범주형(유니크값 < 10):\", cat_cols_by_cardinality)\n",
    "print(\"수치형(유니크값 >= 10):\", num_cols_by_cardinality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8814f-2c4e-4e43-b31c-2dbff63d15f7",
   "metadata": {},
   "source": [
    "## 결측치도 의미가 있을 수 있으므로, 보간하지 않고, MISSING으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edbfec4-b636-42bb-a3bd-c79189818c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_cols_by_cardinality:\n",
    "    Data_EDA[c] = Data_EDA[c].fillna(\"MISSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51379a8-a464-47a4-be19-f4144355bc7d",
   "metadata": {},
   "source": [
    "## 범주형 정보 레이블 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb4d5d4-d8bc-4874-8e74-6f29d98cbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n"
     ]
    }
   ],
   "source": [
    "# 범주형 컬럼 List에서 Target 컬럼을 제외함\n",
    "cat_cols_by_cardinality_less_target = [x for x in cat_cols_by_cardinality if x != 'TARGET']\n",
    "\n",
    "print(cat_cols_by_cardinality_less_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0336e8e1-2e7a-4437-bcf4-ac1e03d4132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder를 보관할 딕셔너리 (각 컬럼별로 학습된 encoder를 저장)\n",
    "Data_EDA_labeled = Data_EDA.copy()\n",
    "encoders = {}\n",
    "# 각 범주형 컬럼에 대해 LabelEncoder로 변환\n",
    "for col in cat_cols_by_cardinality_less_target:\n",
    "    le = LabelEncoder()\n",
    "    Data_EDA_labeled[col] = le.fit_transform(Data_EDA[col])\n",
    "    encoders[col] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78588e8-fdd7-4344-8cfc-c59bf3ce4574",
   "metadata": {},
   "source": [
    "## 수치형 데이터 보간 및 스케일링\n",
    "- Iterative imputer\n",
    "- robust scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16db4c4a-3e11-463d-baf2-928ecf29dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "num_data = Data_EDA[num_cols_by_cardinality].values\n",
    "\n",
    "imp = IterativeImputer(max_iter=10, random_state=42) #하이퍼 파라미터에 대하여 추가적으로 시험해볼 것\n",
    "num_data_mice = imp.fit_transform(num_data)\n",
    "\n",
    "Data_mice = Data_EDA_labeled.copy()\n",
    "Data_mice[num_cols_by_cardinality] = num_data_mice\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "Data_EDA_scaled_mice = Data_mice.copy()\n",
    "Data_EDA_scaled_mice[num_cols_by_cardinality] = scaler.fit_transform(Data_mice[num_cols_by_cardinality])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13730b0d-5ed2-4ee2-854d-d49fcba31617",
   "metadata": {},
   "source": [
    "# Transformer 기반 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29606c4-f606-4ed9-bf31-f35c2b42cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1121dae-f853-422e-95f1-f6854e21be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 246008 Test size: 61503\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(Data_EDA_scaled, test_size=0.2, random_state=42, stratify=Data_EDA_scaled[\"TARGET\"])\n",
    "\n",
    "X_train_cat = train_df[cat_cols_by_cardinality_less_target].values  # (행, 50)\n",
    "X_train_num = train_df[num_cols_by_cardinality].values  # (행, 45)\n",
    "y_train = train_df[\"TARGET\"].values\n",
    "\n",
    "X_test_cat = test_df[cat_cols_by_cardinality_less_target].values\n",
    "X_test_num = test_df[num_cols_by_cardinality].values\n",
    "y_test = test_df[\"TARGET\"].values\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b63601-39ee-4072-b3eb-52aa2913929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246008, 51)\n",
      "(246008, 69)\n",
      "(246008,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_cat.shape)\n",
    "print(X_train_num.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce87032-e24f-49f2-b929-f57855a580dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X_cat, X_num, y):\n",
    "        super().__init__()\n",
    "        self.X_cat = X_cat\n",
    "        self.X_num = X_num\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cat_feats = self.X_cat[idx]  # shape: (51,)\n",
    "        num_feats = self.X_num[idx]  # shape: (69,)\n",
    "        label = self.y[idx]\n",
    "        return {\n",
    "            \"cat\": torch.tensor(cat_feats, dtype=torch.long),\n",
    "            \"num\": torch.tensor(num_feats, dtype=torch.float),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TabularDataset(X_train_cat, X_train_num, y_train)\n",
    "test_dataset = TabularDataset(X_test_cat, X_test_num, y_test)\n",
    "\n",
    "batch_size = 32  # 미니 배치 사이즈를 32로 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2216d2ab-b0f7-401e-9eca-3012c73af844",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_max_dict = {}\n",
    "for i, c in enumerate(cat_cols_by_cardinality_less_target):\n",
    "    num_cats = Data_EDA_scaled[c].max() + 1  # 최대 인덱스 + 1\n",
    "    cat_max_dict[i] = num_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefae4ae-a4ad-470b-bd5f-5c9cef260784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{0: 2, 1: 3, 2: 2, 3: 2, 4: 8, 5: 8, 6: 5, 7: 6, 8: 6, 9: 2, 10: 2, 11: 2, 12: 2, 13: 2, 14: 2, 15: 19, 16: 3, 17: 3, 18: 7, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2, 25: 58, 26: 5, 27: 4, 28: 8, 29: 3, 30: 10, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 2, 37: 2, 38: 2, 39: 2, 40: 2, 41: 2, 42: 2, 43: 2, 44: 2, 45: 2, 46: 2, 47: 2, 48: 2, 49: 2, 50: 2}\n"
     ]
    }
   ],
   "source": [
    "print(type(cat_max_dict))\n",
    "print(cat_max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eee3886-0583-497f-a7b8-d4d81c4bf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bd3e9a8-1da3-47c6-ba47-0e66f9e5d9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5439, 6.1936])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd632e84-d255-429b-9f1f-937c0486ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6630\n",
      "   -> Test Accuracy: 0.7089, ROC-AUC: 0.6885, F1-score: 0.2366\n",
      "Epoch 2/10, Loss: 0.6325\n",
      "   -> Test Accuracy: 0.7356, ROC-AUC: 0.6905, F1-score: 0.2525\n",
      "Epoch 3/10, Loss: 0.6254\n",
      "   -> Test Accuracy: 0.7192, ROC-AUC: 0.7172, F1-score: 0.2519\n",
      "Epoch 4/10, Loss: 0.6177\n",
      "   -> Test Accuracy: 0.7674, ROC-AUC: 0.7259, F1-score: 0.2651\n",
      "Epoch 5/10, Loss: 0.6120\n",
      "   -> Test Accuracy: 0.7040, ROC-AUC: 0.7269, F1-score: 0.2551\n",
      "Epoch 6/10, Loss: 0.6114\n",
      "   -> Test Accuracy: 0.7198, ROC-AUC: 0.7247, F1-score: 0.2586\n",
      "Epoch 7/10, Loss: 0.6095\n",
      "   -> Test Accuracy: 0.7703, ROC-AUC: 0.7306, F1-score: 0.2715\n",
      "Epoch 8/10, Loss: 0.6077\n",
      "   -> Test Accuracy: 0.7243, ROC-AUC: 0.7347, F1-score: 0.2656\n",
      "Epoch 9/10, Loss: 0.6056\n",
      "   -> Test Accuracy: 0.7897, ROC-AUC: 0.7370, F1-score: 0.2796\n",
      "Epoch 10/10, Loss: 0.6048\n",
      "   -> Test Accuracy: 0.7535, ROC-AUC: 0.7436, F1-score: 0.2744\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "class TabTransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_cat_features: int,\n",
    "                 cat_max_dict: dict,\n",
    "                 num_num_features: int,\n",
    "                 d_model: int = 32,\n",
    "                 nhead: int = 4,\n",
    "                 num_layers: int = 2,\n",
    "                 dim_feedforward: int = 64,\n",
    "                 final_hidden: int = 128,\n",
    "                 dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_cat_features = num_cat_features\n",
    "        self.num_num_features = num_num_features\n",
    "        \n",
    "        # 범주형 임베딩\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(int(cat_max_dict[i]), d_model) for i in range(num_cat_features)\n",
    "        ])\n",
    "        \n",
    "        # 수치형 컬럼별 임베딩 (각 컬럼을 d_model 차원으로 변환)\n",
    "        self.numeric_embeddings = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(num_num_features)\n",
    "        ])\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = nn.Embedding(num_cat_features + num_num_features, d_model)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 최종 Fully Connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, final_hidden),\n",
    "            nn.BatchNorm1d(final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(final_hidden, final_hidden),\n",
    "            nn.BatchNorm1d(final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(final_hidden, 2)  # 이진 분류 (로짓 출력)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat: torch.Tensor, x_num: torch.Tensor) -> torch.Tensor:\n",
    "        # 범주형 임베딩\n",
    "        cat_emb = torch.cat([self.embeddings[i](x_cat[:, i]).unsqueeze(1) for i in range(self.num_cat_features)], dim=1)\n",
    "        \n",
    "        # 수치형 데이터 임베딩 (각 컬럼별로 d_model 차원 변환)\n",
    "        num_emb = torch.cat([self.numeric_embeddings[i](x_num[:, i].unsqueeze(1)).unsqueeze(1) for i in range(self.num_num_features)], dim=1)\n",
    "        \n",
    "        # 범주형 + 수치형 결합 (멀티 토큰 형태)\n",
    "        combined_emb = torch.cat([cat_emb, num_emb], dim=1)\n",
    "      \n",
    "        \n",
    "        # Transformer 인코딩\n",
    "        encoded = self.transformer(combined_emb)\n",
    "        summary = torch.mean(encoded, dim=1)  # 평균 풀링\n",
    "        \n",
    "        # 최종 분류\n",
    "        logits = self.fc(summary)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 학습 및 평가\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TabTransformerModel(\n",
    "    num_cat_features=len(cat_cols_by_cardinality_less_target),\n",
    "    cat_max_dict=cat_max_dict,\n",
    "    num_num_features=len(num_cols_by_cardinality),\n",
    "    d_model=32\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        cat_feats = batch[\"cat\"].to(device)\n",
    "        num_feats = batch[\"num\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(cat_feats, num_feats)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    lr_scheduler.step(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # 평가 모드\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            cat_feats = batch[\"cat\"].to(device)\n",
    "            num_feats = batch[\"num\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            logits = model(cat_feats, num_feats)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_prob.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"   -> Test Accuracy: {acc:.4f}, ROC-AUC: {roc_auc:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e3767-cab0-484d-8068-5f7036c61887",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Auto ML 추가\n",
    "- 위에서의 학습량이 부족하여, 하이퍼파라미터 최적화와 epoch를 늘리기 위하여 AutoML 기획하였지만,\n",
    "- 리소스 한계로 최종까지 진행 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5029a3a9-b0ae-4720-8cee-7333a9da3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (215257, 120), Validation: (30751, 120), Test: (61503, 120)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 데이터셋 (X, y)\n",
    "X = Data_EDA_scaled_mice.drop(columns=[\"TARGET\"])  # 특징 데이터\n",
    "y = Data_EDA_scaled_mice[\"TARGET\"]  # 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d87e304-d65f-4f70-a692-b39748d7b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_max_dict = {}\n",
    "for i, c in enumerate(cat_cols_by_cardinality_less_target):\n",
    "    num_cats = Data_EDA_scaled_mice[c].max() + 1  # 최대 인덱스 + 1\n",
    "    cat_max_dict[i] = num_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbfe2368-4a8a-433b-a96c-fbddb5153399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{0: 2, 1: 3, 2: 2, 3: 2, 4: 8, 5: 8, 6: 5, 7: 6, 8: 6, 9: 2, 10: 2, 11: 2, 12: 2, 13: 2, 14: 2, 15: 19, 16: 3, 17: 3, 18: 7, 19: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2, 25: 58, 26: 5, 27: 4, 28: 8, 29: 3, 30: 10, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 2, 37: 2, 38: 2, 39: 2, 40: 2, 41: 2, 42: 2, 43: 2, 44: 2, 45: 2, 46: 2, 47: 2, 48: 2, 49: 2, 50: 2}\n"
     ]
    }
   ],
   "source": [
    "print(type(cat_max_dict))\n",
    "print(cat_max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bef1f0be-0e93-45fe-bc08-01687b5aa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ca968e2-c7d8-4b57-a620-47eab6a7e0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5439, 6.1934])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeaaac-9cd3-4adb-aa67-60e44115959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 23:34:36,401] A new study created in memory with name: no-name-74a65536-8e9a-46ec-bf8d-a0e8923c01d3\n",
      "C:\\Users\\odw94\\AppData\\Local\\Temp\\ipykernel_18260\\2726044733.py:99: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
      "C:\\Users\\odw94\\AppData\\Local\\Temp\\ipykernel_18260\\2726044733.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TabTransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_cat_features: int,\n",
    "                 cat_max_dict: dict,\n",
    "                 num_num_features: int,\n",
    "                 d_model: int = 32,\n",
    "                 nhead: int = 4,\n",
    "                 num_layers: int = 2,\n",
    "                 dim_feedforward: int = 64,\n",
    "                 final_hidden: int = 128,\n",
    "                 dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_cat_features = num_cat_features\n",
    "        self.num_num_features = num_num_features\n",
    "        \n",
    "        # 범주형 임베딩\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(int(cat_max_dict[i]), d_model) for i in range(num_cat_features)\n",
    "        ])\n",
    "        \n",
    "        # 수치형 컬럼별 임베딩 (각 컬럼을 d_model 차원으로 변환)\n",
    "        self.numeric_embeddings = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(num_num_features)\n",
    "        ])\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = nn.Embedding(num_cat_features + num_num_features, d_model)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 최종 Fully Connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, final_hidden),\n",
    "            nn.BatchNorm1d(final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(final_hidden, final_hidden),\n",
    "            nn.BatchNorm1d(final_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(final_hidden, 2)  # 이진 분류 (로짓 출력)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat: torch.Tensor, x_num: torch.Tensor) -> torch.Tensor:\n",
    "        # 범주형 임베딩\n",
    "        cat_emb = torch.cat([self.embeddings[i](x_cat[:, i].long()).unsqueeze(1) for i in range(self.num_cat_features)], dim=1)\n",
    "        \n",
    "        # 수치형 데이터 임베딩 (각 컬럼별로 d_model 차원 변환)\n",
    "        num_emb = torch.cat([self.numeric_embeddings[i](x_num[:, i].unsqueeze(1)).unsqueeze(1) for i in range(self.num_num_features)], dim=1)\n",
    "        \n",
    "        # 범주형 + 수치형 결합 (멀티 토큰 형태)\n",
    "        combined_emb = torch.cat([cat_emb, num_emb], dim=1)\n",
    "        \n",
    "        # Position embedding 추가\n",
    "        positions = torch.arange(self.num_cat_features + self.num_num_features, device=x_cat.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        combined_emb = combined_emb + pos_emb\n",
    "        \n",
    "        # Transformer 인코딩\n",
    "        encoded = self.transformer(combined_emb)\n",
    "        summary = torch.mean(encoded, dim=1)  # 평균 풀링\n",
    "        \n",
    "        # 최종 분류\n",
    "        logits = self.fc(summary)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Optuna를 사용한 K-Fold Cross Validation 적용\n",
    "\n",
    "def objective(trial):\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [16, 32, 64])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # K-Fold Cross Validation 적용\n",
    "    avg_val_loss = 0\n",
    "    \n",
    "    for train_idx, valid_idx in skf.split(X, y):  # K-Fold 분할 적용\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.tensor(X_train_fold[cat_cols_by_cardinality_less_target].to_numpy(), dtype=torch.long), torch.tensor(X_train_fold[num_cols_by_cardinality].to_numpy(), dtype=torch.float32),torch.tensor(y_train_fold.to_numpy(), dtype=torch.long))\n",
    "        valid_dataset = TensorDataset(torch.tensor(X_valid_fold[cat_cols_by_cardinality_less_target].to_numpy(), dtype=torch.long), torch.tensor(X_valid_fold[num_cols_by_cardinality].to_numpy(), dtype=torch.float32), torch.tensor(y_valid_fold.to_numpy(), dtype=torch.long))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        model = TabTransformerModel(\n",
    "            num_cat_features=len(cat_cols_by_cardinality_less_target),\n",
    "            cat_max_dict=cat_max_dict,\n",
    "            num_num_features=len(num_cols_by_cardinality),\n",
    "            d_model=d_model,\n",
    "            num_layers=num_layers,\n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0, 1]), y=y_train_fold)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in train_loader:\n",
    "                cat_feats, num_feats, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(cat_feats.to(device), num_feats.to(device))\n",
    "                loss = criterion(logits, labels.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                cat_feats, num_feats, labels = batch\n",
    "                logits = model(cat_feats.to(device), num_feats.to(device))\n",
    "                val_loss += criterion(logits, labels.to(device)).item()\n",
    "        \n",
    "        avg_val_loss += val_loss / len(valid_loader)\n",
    "    \n",
    "    return avg_val_loss / 5  # 평균 Validation Loss 반환\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a95143-b7e5-4df8-907c-ca29d08f79c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
